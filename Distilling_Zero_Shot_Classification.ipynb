{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Distilling Zero Shot Classification.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "65e7c2a2bfa84bb5af462e7525a2c8db": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_39b11c030de449bc892afa1f1e5f36e2",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_8b8583289b2a40e2b67d073f6a20507d",
              "IPY_MODEL_90386ac483ca4e8983fc273c2ab4126c"
            ]
          }
        },
        "39b11c030de449bc892afa1f1e5f36e2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "8b8583289b2a40e2b67d073f6a20507d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_b89937e4985e49feaf68d03578043a90",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 238,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 238,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_cfa8908bcc924a949db98fa53be4540f"
          }
        },
        "90386ac483ca4e8983fc273c2ab4126c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_c48bd1b53b1b4206adaf20d0c0759e93",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 238/238 [03:15&lt;00:00,  1.22it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_61f3ff8b29014543be0bc2bb8be2ab87"
          }
        },
        "b89937e4985e49feaf68d03578043a90": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "cfa8908bcc924a949db98fa53be4540f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c48bd1b53b1b4206adaf20d0c0759e93": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "61f3ff8b29014543be0bc2bb8be2ab87": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "899620103bc4450fbca7e59e45dcbc81": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_0fbd3a194b284c0f87c1fac48ec0d023",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_3030a8a6822c4ef78e5a0a0b8a7a0049",
              "IPY_MODEL_ca78f7fab6854d2bb57747f966bf66bd"
            ]
          }
        },
        "0fbd3a194b284c0f87c1fac48ec0d023": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "3030a8a6822c4ef78e5a0a0b8a7a0049": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_253cbb88dc904c228a23217f5ef76f53",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 60,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 60,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a98d0be96c5b4252b3c9c3954843397d"
          }
        },
        "ca78f7fab6854d2bb57747f966bf66bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_ec04d21385fc4332b7ac285b9c47dca9",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 60/60 [00:11&lt;00:00,  5.33it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_589eee9a1f414760b11587d4cbcb8943"
          }
        },
        "253cbb88dc904c228a23217f5ef76f53": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a98d0be96c5b4252b3c9c3954843397d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ec04d21385fc4332b7ac285b9c47dca9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "589eee9a1f414760b11587d4cbcb8943": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yasamanrazeghi7/BEtraining/blob/master/Distilling_Zero_Shot_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eGgfoKVYnXPk",
        "outputId": "01072be1-e05a-474b-9cf7-dad0b962eb92",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!git clone https://github.com/huggingface/transformers.git\n",
        "!pip install transformers\n",
        "!pip install datasets\n",
        "!pip install torch"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'transformers' already exists and is not an empty directory.\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.4.2)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.43)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.7/dist-packages (1.5.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets) (1.19.5)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.7/dist-packages (from datasets) (2.0.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.1.5)\n",
            "Requirement already satisfied: huggingface-hub<0.1.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.0.7)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2.23.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from datasets) (3.8.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.11.1)\n",
            "Requirement already satisfied: pyarrow>=0.17.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (3.0.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.7/dist-packages (from datasets) (0.8.7)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.3)\n",
            "Requirement already satisfied: tqdm<4.50.0,>=4.27 in /usr/local/lib/python3.7/dist-packages (from datasets) (4.41.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<0.1.0->datasets) (3.0.12)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->datasets) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->datasets) (3.7.4.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.8.1+cu101)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (3.7.4.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch) (1.19.5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wMrUSGMI5MXZ"
      },
      "source": [
        "This notebook demonstrates how to use a script that provides a way to improve the speed and memory performance of a zero-shot classifier by training a more efficient student model from the zero-shot teacher's predictions over an unlabeled dataset.\n",
        "\n",
        "For a given sequence, the zero-shot classification pipeline requires each possible label to be fed through the large NLI model separately. This requirement slows results considerably, particularly for tasks with a large number of classes K.\n",
        "\n",
        "We'll use the AG's News topic classification dataset for this example."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gUCM704I5u6E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "209078f9-3cca-4fc8-af15-6ffdb89507e0"
      },
      "source": [
        "from datasets import load_dataset\n",
        "dev = load_dataset('mc_taco', split='validation')\n",
        "dev[0]"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reusing dataset mc_taco (/root/.cache/huggingface/datasets/mc_taco/plain_text/1.1.0/8ebb9cb3e1a5bb32a5e9478ca6fda5d143d5a267ecde7b907d6b80e03d96e127)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'answer': 'it sometimes was',\n",
              " 'category': 4,\n",
              " 'label': 0,\n",
              " 'question': 'Is Islam still the majority religion?',\n",
              " 'sentence': 'Islam later emerged as the majority religion during the centuries of Ottoman rule, though a significant Christian minority remained.'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p-BoCOrCQ-hk",
        "outputId": "2fe7481e-b8ba-4caa-d2d5-39be84b60f1d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 392
        }
      },
      "source": [
        "import torch\n",
        "roberta = torch.hub.load('pytorch/fairseq', 'roberta.large.mnli')\n",
        "roberta.eval()  # disable dropout for evaluation\n",
        "\n",
        "# Encode a pair of sentences and make a prediction\n",
        "tokens = roberta.encode('Roberta is a heavily optimized version of BERT.', 'Roberta is not very optimized.')\n",
        "roberta.predict('mnli', tokens).argmax()  # 0: contradiction\n"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using cache found in /root/.cache/torch/hub/pytorch_fairseq_master\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-59-1a5cc319c7a5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mroberta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhub\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pytorch/fairseq'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'roberta.large.mnli'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mroberta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# disable dropout for evaluation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Encode a pair of sentences and make a prediction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/hub.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(repo_or_dir, model, *args, **kwargs)\u001b[0m\n\u001b[1;32m    337\u001b[0m         \u001b[0mrepo_or_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_cache_or_reload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrepo_or_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_reload\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 339\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_load_local\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrepo_or_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    340\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/hub.py\u001b[0m in \u001b[0;36m_load_local\u001b[0;34m(hubconf_dir, model, *args, **kwargs)\u001b[0m\n\u001b[1;32m    363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m     \u001b[0mhubconf_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhubconf_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMODULE_HUBCONF\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 365\u001b[0;31m     \u001b[0mhub_module\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMODULE_HUBCONF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhubconf_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    366\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    367\u001b[0m     \u001b[0mentry\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_load_entry_from_hubconf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhub_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/hub.py\u001b[0m in \u001b[0;36mimport_module\u001b[0;34m(name, path)\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule_from_spec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLoader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m     \u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexec_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
            "\u001b[0;32m/root/.cache/torch/hub/pytorch_fairseq_master/hubconf.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mmissing_deps\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmissing_deps\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Missing dependencies: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\", \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmissing_deps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Missing dependencies: hydra-core, omegaconf"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SMZcx0dz61ef"
      },
      "source": [
        "### ðŸ¤— Zero-shot classification pipeline\n",
        "\n",
        "The [zero-shot classification pipeline](https://huggingface.co/transformers/main_classes/pipelines.html#transformers.ZeroShotClassificationPipeline) is a tool withing ðŸ¤— Transformers that can be used to classify text sequences out of the box, provided only a list of possible class names:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZbaEZ1nA69_8",
        "outputId": "810be7f8-a141-4516-ec4c-f3dc78da496c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from transformers import pipeline\n",
        "zero_shot_classifier = pipeline('zero-shot-classification', model=\"roberta-large-mnli\", device=0)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at roberta-large-mnli were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qq0e2D4xJjtx",
        "outputId": "fdf81d7b-0f1e-4432-fca6-09ff4223bda5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "time_l = ['second', 'minute', 'hour', 'day', 'week', 'month', 'year', 'decade', 'century', 'centuries', 'times', 'never', 'daily', 'rarely', 'millions', 'frequently', 'every', 'once', 'twice']\n",
        "def unique(list1):\n",
        "  print(f'The original list was {len(list1)}')\n",
        "  # insert the list to the set\n",
        "  list_set = set(list1)\n",
        "  # convert the set to the list\n",
        "  unique_list = (list(list_set))\n",
        "  print(f'Now changed to {len(unique_list)}')\n",
        "  for x in unique_list:\n",
        "    print(x)\n",
        "ans_values = []\n",
        "for i in range(len(dev)):\n",
        "  if dev[i]['category'] ==  3:\n",
        "    # print(dev[i])\n",
        "    # input()\n",
        "    ans_values.append(dev[i]['answer'])\n",
        "    bool_flag = True\n",
        "    # for tt in time_l:\n",
        "    #   if tt in dev[i]['answer'] :\n",
        "    #     bool_flag = False\n",
        "    if bool_flag:\n",
        "      print(dev[i]['answer'])\n",
        "\n",
        "# unique(ans_values)\n",
        "    \n"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "first sunday\n",
            "later sunday\n",
            "sunday\n",
            "7th month\n",
            "7th day\n",
            "7:00 PM\n",
            "it was morning\n",
            "5:00 PM\n",
            "7:00 AM\n",
            "it was evening\n",
            "3:00 PM\n",
            "saturday\n",
            "friday\n",
            "8:00 PM\n",
            "a second later\n",
            "a hour later\n",
            "7:00 PM\n",
            "a minute before it started\n",
            "7:00 AM\n",
            "midnight\n",
            "5:00 AM\n",
            "at dawn\n",
            "ratner first noticed a profit after expanding their market to the us, one hour ago on april 1st\n",
            "ratner first noticed a profit after expanding their market to the us, one second ago on april 1st\n",
            "2:00 PM\n",
            "1:00 PM\n",
            "it spend on the facility\n",
            "about 12 am\n",
            "about 3 am\n",
            "about 1 am\n",
            "about 2 am\n",
            "detroit began talking about plastic-bodied cars\n",
            "3:00 PM\n",
            "3:00 AM\n",
            "10:00 AM\n",
            "5:00 PM\n",
            "about 5 am\n",
            "around 2 am\n",
            "2:00 AM\n",
            "about 10 am\n",
            "12:00 PM\n",
            "12:00 AM\n",
            "after he left earlier\n",
            "since he left earlier\n",
            "tomorrow\n",
            "before a couple days\n",
            "the movie was released in the past hour\n",
            "3:00 AM\n",
            "12:00 AM\n",
            "5:00 AM\n",
            "9:00 PM\n",
            "when he was a child\n",
            "once he got to washington\n",
            "1:00 AM\n",
            "1:00 PM\n",
            "1:00 AM\n",
            "three seconds ago\n",
            "the work gets the award\n",
            "5:00 PM\n",
            "1:00 PM\n",
            "3:30 AM\n",
            "midnight\n",
            "3:30 PM\n",
            "4:00 PM\n",
            "2:00 AM\n",
            "1:00 AM\n",
            "noon\n",
            "2:00 PM\n",
            "6:00 PM\n",
            "8:00 PM\n",
            "at midnight\n",
            "never\n",
            "4:00 PM\n",
            "10:00 AM\n",
            "2:00 AM\n",
            "midnight\n",
            "3:00 PM\n",
            "the subpoena will be granted before the committee is granted authority to grant subpoenas\n",
            "the subpoena will be granted after the committee is granted authority to grant subpoenas\n",
            "the subpoena will be granted 207 years from now\n",
            "the intelligence committees exercise influence\n",
            "always\n",
            "when asked\n",
            "sometimes\n",
            "4:00 PM\n",
            "pushkin liked to speak at 2:45 a.m. on saturday mornings\n",
            "pushkin liked to speak at high noon usually\n",
            "5:00 AM\n",
            "5:00 PM\n",
            "12:00 PM\n",
            "4:00 AM\n",
            "8:00 PM\n",
            "1:00 PM\n",
            "5:00 AM\n",
            "3:00 AM\n",
            "4:00 PM\n",
            "9:00 AM\n",
            "1:00 AM\n",
            "noon\n",
            "3:00 PM\n",
            "at 16:00\n",
            "10:00 AM\n",
            "5:00 PM\n",
            "midnight\n",
            "at 5 pm\n",
            "at midnight\n",
            "at 5 am\n",
            "in the afternoon\n",
            "she doubted that kind of performance is sustainable\n",
            "right now\n",
            "within a few hours\n",
            "within a few weeks\n",
            "in an hour\n",
            "in an week\n",
            "within a few seconds\n",
            "at midnight\n",
            "12:00 AM\n",
            "in 2115\n",
            "10:00 PM\n",
            "1:00 AM\n",
            "6:00 AM\n",
            "3:30 AM\n",
            "four minutes ago\n",
            "tomorrow\n",
            "four centuries ago\n",
            "before they laughed\n",
            "the next night\n",
            "the next day\n",
            "the next century\n",
            "the next month\n",
            "monday\n",
            "tuesday\n",
            "on monday\n",
            "4:00 PM\n",
            "in the evening\n",
            "in the morning\n",
            "6:00 PM\n",
            "1:00 PM\n",
            "3:00 PM\n",
            "175\n",
            "7\n",
            "3\n",
            "63\n",
            "45\n",
            "age 95\n",
            "age 57\n",
            "750\n",
            "105\n",
            "150\n",
            "age 19\n",
            "75\n",
            "7.5\n",
            "3 week before he turned 39 years old\n",
            "the first friday of each week\n",
            "1 week before he turned 39 years old\n",
            "this evening\n",
            "the first friday of each month\n",
            "the first friday of each century\n",
            "the first friday of each year\n",
            "1 week before he turned 195 years old\n",
            "1 week after he turned 39 years old\n",
            "the second friday of each month\n",
            "the third friday of each month\n",
            "1 week before he turned 7.8 years old\n",
            "1:00 PM\n",
            "2:00 PM\n",
            "11:00 PM\n",
            "12:00 AM\n",
            "a month ago\n",
            "a months ago\n",
            "at 12 am\n",
            "until midnight\n",
            "at 10 pm\n",
            "10:00 AM\n",
            "a day ago\n",
            "a year ago\n",
            "at midnight\n",
            "2:00 AM\n",
            "6:00 AM\n",
            "a century ago\n",
            "after midnight\n",
            "in the last 15 minutes\n",
            "in the first 5 minutes\n",
            "in the last 5 minutes\n",
            "in the last 2.5 minutes\n",
            "the nsc arrived in march of 1999\n",
            "6:00 AM\n",
            "2:00 PM\n",
            "5:00 PM\n",
            "10:00 AM\n",
            "a few seconds ago\n",
            "a few days ago\n",
            "a few centuries ago\n",
            "a few weeks ago\n",
            "at 8am\n",
            "180 years ago\n",
            "20 seconds ago\n",
            "approximately 10 am\n",
            "at 11 a.m\n",
            "about 12 am\n",
            "about 10 pm\n",
            "4:00 PM\n",
            "4:00 AM\n",
            "about 1 am\n",
            "approximately 12 am\n",
            "1:00 AM\n",
            "around 10 am\n",
            "at 1 a.m\n",
            "at 12 a.m\n",
            "12:00 PM\n",
            "2:00 AM\n",
            "about 10 am\n",
            "around 12 am\n",
            "2:00 PM\n",
            "4:00 PM\n",
            "two years from now\n",
            "2:00 PM\n",
            "6:00 AM\n",
            "12:00 PM\n",
            "two seconds from now\n",
            "about 13 am\n",
            "11:00 PM\n",
            "about 1 am\n",
            "2:00 AM\n",
            "around 1 am\n",
            "after it lowered its head\n",
            "when it walked away\n",
            "after it lifted its head\n",
            "when it walks away\n",
            "2:00 PM\n",
            "before it lifted its head\n",
            "after it raised its head\n",
            "when he became 0.67 months old\n",
            "when he became 0.67 years old\n",
            "yesterday\n",
            "in the 1900s\n",
            "tomorrow\n",
            "1910\n",
            "the early 1900s\n",
            "1960\n",
            "2012\n",
            "a few minutes in\n",
            "at the five days mark\n",
            "at the five minute mark\n",
            "at the start\n",
            "and the very end\n",
            "at the five month mark\n",
            "at the very ends\n",
            "at the five day time\n",
            "a cut centuries in\n",
            "at the beginning\n",
            "tomorrow\n",
            "yesterday\n",
            "10 hours ago\n",
            "10 seconds ago\n",
            "10 years ago\n",
            "4:00 PM\n",
            "3 p.m\n",
            "2 a.m\n",
            "2:00 AM\n",
            "2:00 PM\n",
            "5:00 PM\n",
            "12:00 PM\n",
            "4:00 PM\n",
            "no time of day\n",
            "3 p.m\n",
            "1 p.m\n",
            "never\n",
            "3:00 PM\n",
            "12:00 PM\n",
            "when his father was 21\n",
            "90 years ago\n",
            "yesterday\n",
            "10 hours ago\n",
            "10 seconds ago\n",
            "when his father was 7\n",
            "2001\n",
            "30 years ago\n",
            "3:00 AM\n",
            "6:00 AM\n",
            "12:00 AM\n",
            "5:00 PM\n",
            "2:00 PM\n",
            "4:00 AM\n",
            "6:00 PM\n",
            "5:00 AM\n",
            "afternoon\n",
            "8:00 PM\n",
            "2:00 AM\n",
            "from 9am to 7 pm\n",
            "from 9am to 5 pm\n",
            "10:00 PM\n",
            "from 9am to 5 am\n",
            "one minute ago\n",
            "5:00 PM\n",
            "9:00 PM\n",
            "8:00 AM\n",
            "2:00 PM\n",
            "from 9am to 4 pm\n",
            "4:00 PM\n",
            "3:00 PM\n",
            "1:00 PM\n",
            "5:00 PM\n",
            "12:00 PM\n",
            "8:00 PM\n",
            "7:00 AM\n",
            "2:00 AM\n",
            "9:00 PM\n",
            "2:00 PM\n",
            "1:00 AM\n",
            "11:00 AM\n",
            "8:00 AM\n",
            "1:00 PM\n",
            "about 12 am\n",
            "4:00 PM\n",
            "2:00 AM\n",
            "around 6 pm\n",
            "3:00 PM\n",
            "in the 2nd century ad\n",
            "1000 seconds ago\n",
            "in the 2nd month ad\n",
            "in the 2nd minute ad\n",
            "during the 1970s\n",
            "1000 weeks ago\n",
            "in the 1st century ad\n",
            "50 years ago\n",
            "1000 years ago\n",
            "in the 1970s\n",
            "when his was born\n",
            "90 years ago\n",
            "150 years ago\n",
            "100 years ago\n",
            "when he was born\n",
            "30 seconds ago\n",
            "60 years ago\n",
            "mornings\n",
            "every time they looked at her\n",
            "once a century\n",
            "once a minute\n",
            "saturday\n",
            "today\n",
            "once a second\n",
            "once a year\n",
            "yesterday\n",
            "3 months ago\n",
            "27 months ago\n",
            "1 months ago\n",
            "9 months ago\n",
            "around 10 am\n",
            "at midnight\n",
            "approximately 10 am\n",
            "at 2 p.m\n",
            "about 10 am\n",
            "2:00 AM\n",
            "1:00 AM\n",
            "10:00 AM\n",
            "2:00 PM\n",
            "at dawn\n",
            "during the month\n",
            "during the century\n",
            "during the night\n",
            "they went to the dog pound 15 years ago, in may\n",
            "during the day\n",
            "at bedtime\n",
            "they went to the dog pound 45 years ago, in may\n",
            "they went to the dog pound last saturday\n",
            "at 8 a.m\n",
            "9:00 PM\n",
            "at 2 a.m\n",
            "next year\n",
            "at 6 a.m\n",
            "1:00 AM\n",
            "3:00 AM\n",
            "3:00 PM\n",
            "1:00 PM\n",
            "1994\n",
            "1996\n",
            "mzoudi studied in dortmund in 1990\n",
            "mzoudi studied in dortmund in 1980\n",
            "1987\n",
            "in 5589\n",
            "the next century\n",
            "in 2014\n",
            "yesterday\n",
            "in 6042\n",
            "the next minute\n",
            "a second later\n",
            "11 p.m\n",
            "10:00 AM\n",
            "10:30 AM\n",
            "3:00 AM\n",
            "5:00 AM\n",
            "10 p.m\n",
            "5:00 PM\n",
            "10:00 PM\n",
            "5 p.m\n",
            "10:30 PM\n",
            "3:00 PM\n",
            "spring\n",
            "1995\n",
            "2017\n",
            "2015\n",
            "90 years ago\n",
            "30 years ago\n",
            "he said it 's certainly not too low\n",
            "1:00 PM\n",
            "2:00 PM\n",
            "5:00 AM\n",
            "4:00 PM\n",
            "1:00 AM\n",
            "3:00 AM\n",
            "low-income people own their own homes\n",
            "12:00 PM\n",
            "2:00 AM\n",
            "6:00 AM\n",
            "2:00 PM\n",
            "3:00 AM\n",
            "10:00 AM\n",
            "5:00 AM\n",
            "he declined to be specific\n",
            "in the afternoon\n",
            "about noon\n",
            "8:00 AM\n",
            "the french revolutioin began in 1790\n",
            "20 years ago\n",
            "the french revolutioin began in 1960\n",
            "may 4th\n",
            "may 1st\n",
            "the french revolutioin started in 1790\n",
            "may 5th, 1789\n",
            "1989\n",
            "100 years ago\n",
            "1900\n",
            "5700\n",
            "100 hours ago\n",
            "tomorrow\n",
            "yesterday\n",
            "100 seconds ago\n",
            "17100\n",
            "400 ad\n",
            "2:00 AM\n",
            "the store usually opens at 9 a.m. in the morning\n",
            "at 10 am\n",
            "at 10 pm\n",
            "7:00 PM\n",
            "the store usually opens at 3 a.m. in the morning\n",
            "2:00 PM\n",
            "the store usually opens at 3:30 a.m. at day\n",
            "1983\n",
            "1981\n",
            "1999\n",
            "in 1981\n",
            "2 minutes ago\n",
            "april 5th 670\n",
            "never\n",
            "2 weeks ago\n",
            "10:00 AM\n",
            "at midnight\n",
            "8:00 AM\n",
            "11:00 AM\n",
            "at 9 am\n",
            "2 month ago\n",
            "10 hour ago\n",
            "10 months ago\n",
            "2 years ago\n",
            "0.56 minutes ago\n",
            "0.56 days ago\n",
            "90 years ago\n",
            "5 minutes ago\n",
            "2 minute ago\n",
            "5 months ago\n",
            "20 years ago\n",
            "the shelter did not answer\n",
            "1 hr ago\n",
            "180 years ago\n",
            "3 hr ago\n",
            "2:00 PM\n",
            "60 years ago\n",
            "tumble ate the oatmeal around 1am\n",
            "tumble eaten the oatmeal at 1am\n",
            "tumble ate his oatmeal at 1am\n",
            "tumble consumed the oatmeal at 1am\n",
            "tumble ate the oatmeal about 1am\n",
            "tumble ate the oatmeal at 1am\n",
            "one century ago\n",
            "8:00 AM\n",
            "1:00 AM\n",
            "9:00 PM\n",
            "10:00 PM\n",
            "1:00 PM\n",
            "3:00 PM\n",
            "at 8am\n",
            "10:00 AM\n",
            "8:00 AM\n",
            "3:00 AM\n",
            "12:00 AM\n",
            "1:00 AM\n",
            "2:00 AM\n",
            "2:00 PM\n",
            "3:00 PM\n",
            "4:00 AM\n",
            "4:00 PM\n",
            "early evening\n",
            "weekends\n",
            "early morning\n",
            "2:00 PM\n",
            "early late night\n",
            "12:00 PM\n",
            "at 5 pm everyday\n",
            "tomorrow\n",
            "he starts work at 9:am\n",
            "he starts work when 9 : am\n",
            "he begins work at 12 : am\n",
            "10 p.m\n",
            "9:00 AM\n",
            "he starts work at 12:am\n",
            "he starts working at 12 : am\n",
            "he starts working at 4 : am\n",
            "he begins work at 9 : am\n",
            "8 a.m\n",
            "he starts work after 9 : am\n",
            "he starts work around 12 : am\n",
            "he starts working at 9 : pm\n",
            "he starts work at 9 : am\n",
            "he starts work when 9 : pm\n",
            "eight a . m\n",
            "he starts work when 12 : am\n",
            "he starts work around 4 : am\n",
            "sometime yesterday\n",
            "the poor began to lose services after she lost the funding\n",
            "a few centuries ago\n",
            "after the new bill passed\n",
            "a few days ago\n",
            "a few months ago\n",
            "5:00 PM\n",
            "11:00 AM\n",
            "day\n",
            "the girl saw the bear on a tuesday afternoon\n",
            "the girl spotted the bear on a tuesday afternoon\n",
            "the girl saw the bear on a sunday afternoon\n",
            "tomorrow\n",
            "over the last 5 minutes\n",
            "it occurred around noon\n",
            "12:00 AM\n",
            "7:00 PM\n",
            "3:00 AM\n",
            "7:00 AM\n",
            "9:00 AM\n",
            "1:00 PM\n",
            "1:00 AM\n",
            "it occurred around midnight\n",
            "12:00 PM\n",
            "4:00 AM\n",
            "8:00 PM\n",
            "7:00 AM\n",
            "5:00 PM\n",
            "11:00 PM\n",
            "8:00 AM\n",
            "9:00 AM\n",
            "7:00 PM\n",
            "10:00 PM\n",
            "11:00 AM\n",
            "it only took a few minutes\n",
            "15 minutes\n",
            "15 months\n",
            "it took 2 centuries to count all the votes\n",
            "10:00 PM\n",
            "monday\n",
            "tuesday\n",
            "wednesday\n",
            "3:00 AM\n",
            "when office opens at 8am\n",
            "when she was gone\n",
            "a few months ago\n",
            "a few weeks ago\n",
            "tomorrow\n",
            "yesterday\n",
            "a few seconds ago\n",
            "about an hour ago\n",
            "a few hours ago\n",
            "he ran off instead\n",
            "in the late night\n",
            "0.33 am\n",
            "mr. guber jumped at the chance\n",
            "at noon today\n",
            "he chasing a story\n",
            "3:00 AM\n",
            "he ducks\n",
            "they need to belong to a health club\n",
            "monday also\n",
            "every last monday in the year\n",
            "they need all\n",
            "monday\n",
            "tuesday\n",
            "every first monday in the year\n",
            "every first monday in the hour\n",
            "3:00 AM\n",
            "6:00 AM\n",
            "11:00 PM\n",
            "midnight\n",
            "1:00 AM\n",
            "11:00 AM\n",
            "4:00 AM\n",
            "12:00 AM\n",
            "8:00 PM\n",
            "at 6 pm\n",
            "at midnight\n",
            "tuesday evening\n",
            "5:00 AM\n",
            "junior graduates from high school\n",
            "at 2 am\n",
            "monday evening\n",
            "at 6 am\n",
            "7:00 AM\n",
            "at 1 am\n",
            "2:00 AM\n",
            "12:00 AM\n",
            "sunday evening\n",
            "8:00 AM\n",
            "10:00 AM\n",
            "his son paid\n",
            "0.5 month ago\n",
            "9:00 AM\n",
            "0.2 month ago\n",
            "0.6 day ago\n",
            "11:00 AM\n",
            "9:00 AM\n",
            "in the afternoon\n",
            "in the morning\n",
            "10:00 AM\n",
            "2:00 PM\n",
            "at midnight\n",
            "12:00 AM\n",
            "friday 29/7/1994\n",
            "9:00 AM\n",
            "2:00 AM\n",
            "4:00 AM\n",
            "11:00 AM\n",
            "9:00 PM\n",
            "1:00 PM\n",
            "noon\n",
            "12 years ago\n",
            "20 years ago\n",
            "5 months ago\n",
            "5 minutes ago\n",
            "last year\n",
            "4 seconds ago\n",
            "last second\n",
            "2.5 minutes ago\n",
            "4 years ago\n",
            "when he was a child\n",
            "3:00 AM\n",
            "last year\n",
            "last season\n",
            "550\n",
            "one of the early weeks of the fourth century\n",
            "one of the late years of the fourth century\n",
            "he hasn't arrived yet\n",
            "he refused\n",
            "3 centuries\n",
            "never\n",
            "always\n",
            "next minute\n",
            "next day\n",
            "following month\n",
            "6:00 AM\n",
            "noon\n",
            "2:00 AM\n",
            "6:00 AM\n",
            "7:00 PM\n",
            "6:00 PM\n",
            "11:00 PM\n",
            "5:00 PM\n",
            "10 years before\n",
            "this evening\n",
            "10 hours ago\n",
            "this morning\n",
            "40 hours ago\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Y_Inif5DjAV",
        "outputId": "399485fa-c356-4b62-99aa-6437103395cc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        }
      },
      "source": [
        "t_count = 0\n",
        "class_names = [\"true\", \"false\"]\n",
        "for i in range(len(dev)):\n",
        "  a = zero_shot_classifier([dev[i]['sentence'] + \" \" + dev[i]['question'], dev[i]['answer']])\n",
        "  print(a)\n",
        "  ans = a['scores']\n",
        "  if ans[0] > ans[1]:\n",
        "    p_label = 1\n",
        "  else:\n",
        "    p_label = 0\n",
        "  if p_label == dev[i]['label']:\n",
        "    t_count = t_count + 1\n",
        "print(t_count)\n",
        "\n"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-38-936e07df5dbe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mclass_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"true\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"false\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdev\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m   \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzero_shot_classifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdev\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sentence'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdev\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'question'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'answer'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0mans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'scores'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: __call__() missing 1 required positional argument: 'candidate_labels'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zM5h4goJGNzF"
      },
      "source": [
        "from transformers import pipeline\n",
        "zero_shot_classifier = pipeline('zero-shot-classification', model=\"roberta-large-mnli\", device=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y8_z-awr9LEb",
        "outputId": "6982c562-d142-40d3-c231-01f3d0516e4c"
      },
      "source": [
        "sequence = \"A new moon has been discovered in Jupiter's orbit.\"\n",
        "class_names = [\"the world\", \"sports\", \"business\", \"science/tech\"]\n",
        "zero_shot_classifier(sequence, class_names)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'labels': ['science/tech', 'the world', 'business', 'sports'],\n",
              " 'scores': [0.7967991232872009,\n",
              "  0.08699782937765121,\n",
              "  0.07631051540374756,\n",
              "  0.03989255428314209],\n",
              " 'sequence': \"A new moon has been discovered in Jupiter's orbit.\"}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QCBuMl9rD9yA"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AB-0v4ep84vv"
      },
      "source": [
        "This method serves as a convenient out-of-the-box classifier. Unfortunately, the method is by necessity somewhat slow. This is partially due to the large underlying model being used, but more important is the fact that for this method to work, every possible sequence / class name pair must be fed through the model together. So in order to classify `N` sequences into `K` classes, the model has to be called `N*K` times (whereas a typical classifier would only be called `N` times). This makes the method comparatively slow, especially for settings with a large number of classes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7h2uvu0F90_9",
        "outputId": "b4d7f004-29cf-422f-fc80-0bd84dc519f3"
      },
      "source": [
        "# classify 1600 examples with K=4 classes\n",
        "%%time\n",
        "for _ in range(100):\n",
        "    zero_shot_classifier([sequence] * 16, class_names)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 6.23 s, sys: 2.83 s, total: 9.06 s\n",
            "Wall time: 8.95 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5-rBuO1_973y",
        "outputId": "cd53f383-c4e1-4047-ad10-a201060f978d"
      },
      "source": [
        "# classify 1600 examples with K=8 classes\n",
        "%%time\n",
        "expanded_class_names = class_names + [\"politics\", \"health\", \"food\", \"weather\"]\n",
        "for _ in range(100):\n",
        "    zero_shot_classifier([sequence] * 16, expanded_class_names)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 10.6 s, sys: 6.96 s, total: 17.6 s\n",
            "Wall time: 17.2 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yTnrdYNI-zR5"
      },
      "source": [
        "As we can see, increasing the number of classes from `K=4` to `K=8` approximately doubles the inference time. This classification method is extremely useful, but ideally we'd like to speed up inference."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QwQz_FPB_K4j"
      },
      "source": [
        "### Distilling a more efficient student model\n",
        "\n",
        "The best way to speed up inference is to **train a more efficient student model on the zero-shot classifier's predictions** over an unlabeled dataset. This can be done with the [`distill_classifier.py`](https://github.com/huggingface/transformers/blob/master/examples/research_projects/zero-shot-distillation/distill_classifier.py) script provided in the `transformers` repo.\n",
        "\n",
        "Given (1) an unlabeled corpus and (2) a set of `K` class names, this script allows a user to train a standard classification head with `K` output dimensions. The script generates a softmax distribution for the provided data & class names, and a student classifier is then fine-tuned on these proxy labels. The resulting student model can be used for classifying novel text instances into the previously specified `K` classes with an order-of-magnitude boost in inference speed plus decreased memory usage.\n",
        "\n",
        "Let's see how to do this with the [AG's News](https://huggingface.co/datasets/ag_news) topic classification dataset. The first thing we need is an unlabeled dataset (in reality AG's News is annotated of course, but we'll pretend and ignore the annotations for the sake of example). Let's put the sequences from the train set into a `txt` file:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xykweYIWoy-6"
      },
      "source": [
        "!mkdir agnews\n",
        "with open(\"agnews/train_unlabeled.txt\", 'w') as f:\n",
        "    for seq in train[\"text\"]:\n",
        "        f.write(seq + '\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B8zXcKPqApoG"
      },
      "source": [
        "The other thing the script needs is the names of the classes. We'll put these into their own newline-delimitted `txt` as well:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Gn5PidvAkHQ"
      },
      "source": [
        "with open(\"agnews/class_names.txt\", 'w') as f:\n",
        "    for label in class_names:\n",
        "        f.write(label + '\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0A11PttrBZgf"
      },
      "source": [
        "Now we can run the script. First the zero-shot model will loop through the data and generate (soft) proxy-labels, and then a student `DistilBert` model will be fine-tuned on these predictions. The student will then be saved in `./distilbert-base-uncased-agnews-student`. See the [script readme](https://github.com/huggingface/transformers/blob/master/examples/research_projects/zero-shot-distillation/README.md) for more information about the available script arguments.\n",
        "\n",
        "On a single P100, this will take about ~2 hours with the full training set of 130K examples. On a V100 with mixed precision (just pass `--fp16`), it will take ~30 minutes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ECt06ndcnpyb",
        "outputId": "a54094b8-83fb-4f69-a8c7-96c515393ee4"
      },
      "source": [
        "!python transformers/examples/research_projects/zero-shot-distillation/distill_classifier.py \\\n",
        "--data_file ./agnews/train_unlabeled.txt \\\n",
        "--class_names_file ./agnews/class_names.txt \\\n",
        "--hypothesis_template \"This text is about {}.\" \\\n",
        "--student_name_or_path distilbert-base-uncased \\\n",
        "--output_dir ./distilbert-base-uncased-agnews-student"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-02-18 19:12:46.327937: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
            "02/18/2021 19:12:47 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "02/18/2021 19:12:47 - INFO - __main__ -   Training/evaluation parameters DistillTrainingArguments(output_dir='./agnews/distilled_model', overwrite_output_dir=False, do_train=True, do_eval=True, do_predict=False, evaluation_strategy=<EvaluationStrategy.NO: 'no'>, prediction_loss_only=False, per_device_train_batch_size=32, per_device_eval_batch_size=128, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=5e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=1.0, max_steps=-1, lr_scheduler_type=<SchedulerType.LINEAR: 'linear'>, warmup_steps=0, logging_dir='runs/Feb18_19-12-47_6ec4e1e7d6f8', logging_first_step=False, logging_steps=500, save_steps=500, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level='O1', fp16_backend='auto', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name='./agnews/distilled_model', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, sharded_ddp=False, deepspeed=None, label_smoothing_factor=0.0, adafactor=False, group_by_length=False, report_to=['tensorboard'], ddp_find_unused_parameters=None, dataloader_pin_memory=True)\n",
            "02/18/2021 19:12:47 - INFO - __main__ -   Generating predictions from zero-shot teacher model\n",
            "[INFO|configuration_utils.py:456] 2021-02-18 19:12:48,053 >> loading configuration file https://huggingface.co/roberta-large-mnli/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/fab42bdbd5cb5e6ff7cabeb9bcc12728f56022f50b9644a3079904564f2bc704.ddc5961cccf081d6ca7f4f58ee119c21895aa9b19f0044f01954cd2ff42fefcb\n",
            "[INFO|configuration_utils.py:492] 2021-02-18 19:12:48,054 >> Model config RobertaConfig {\n",
            "  \"_num_labels\": 3,\n",
            "  \"architectures\": [\n",
            "    \"RobertaForSequenceClassification\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"CONTRADICTION\",\n",
            "    \"1\": \"NEUTRAL\",\n",
            "    \"2\": \"ENTAILMENT\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"label2id\": {\n",
            "    \"CONTRADICTION\": 0,\n",
            "    \"ENTAILMENT\": 2,\n",
            "    \"NEUTRAL\": 1\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.4.0.dev0\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:1035] 2021-02-18 19:12:48,326 >> loading weights file https://huggingface.co/roberta-large-mnli/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/63cbd98723b89863bcd86a8002e823de3004a139513559246690c65521cdc9b9.38ef55c51c84ab2e78e5a0e2ea9c25830fd074df70d2f10071eb9a1bc1586ca0\n",
            "[WARNING|modeling_utils.py:1143] 2021-02-18 19:13:00,207 >> Some weights of the model checkpoint at roberta-large-mnli were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[INFO|modeling_utils.py:1160] 2021-02-18 19:13:00,207 >> All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at roberta-large-mnli.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.\n",
            "[INFO|configuration_utils.py:456] 2021-02-18 19:13:06,584 >> loading configuration file https://huggingface.co/roberta-large-mnli/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/fab42bdbd5cb5e6ff7cabeb9bcc12728f56022f50b9644a3079904564f2bc704.ddc5961cccf081d6ca7f4f58ee119c21895aa9b19f0044f01954cd2ff42fefcb\n",
            "[INFO|configuration_utils.py:492] 2021-02-18 19:13:06,585 >> Model config RobertaConfig {\n",
            "  \"_num_labels\": 3,\n",
            "  \"architectures\": [\n",
            "    \"RobertaForSequenceClassification\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"CONTRADICTION\",\n",
            "    \"1\": \"NEUTRAL\",\n",
            "    \"2\": \"ENTAILMENT\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"label2id\": {\n",
            "    \"CONTRADICTION\": 0,\n",
            "    \"ENTAILMENT\": 2,\n",
            "    \"NEUTRAL\": 1\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.4.0.dev0\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:1786] 2021-02-18 19:13:07,436 >> loading file https://huggingface.co/roberta-large-mnli/resolve/main/vocab.json from cache at /root/.cache/huggingface/transformers/64a1d72b2bd05b0aff1a4dd9e7a90a6eea0312b4f914e80b0a923aa8f72219bd.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\n",
            "[INFO|tokenization_utils_base.py:1786] 2021-02-18 19:13:07,437 >> loading file https://huggingface.co/roberta-large-mnli/resolve/main/merges.txt from cache at /root/.cache/huggingface/transformers/425529714b758f50b6d3f93f8093d859856fd41cf1cec7c8edf2ab44aee632b6.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
            "[INFO|tokenization_utils_base.py:1786] 2021-02-18 19:13:07,437 >> loading file https://huggingface.co/roberta-large-mnli/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/d077eac6b48c43618a441cba6eab600a5cc6383b98e7eada6d1ad4d3f3cc457e.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\n",
            "100% 15000/15000 [41:52<00:00,  5.97it/s]\n",
            "02/18/2021 19:55:00 - INFO - __main__ -   Initializing student model\n",
            "[INFO|file_utils.py:1316] 2021-02-18 19:55:01,282 >> https://huggingface.co/distilbert-base-uncased/resolve/main/config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp3i47_ytj\n",
            "Downloading: 100% 442/442 [00:00<00:00, 324kB/s]\n",
            "[INFO|file_utils.py:1320] 2021-02-18 19:55:01,554 >> storing https://huggingface.co/distilbert-base-uncased/resolve/main/config.json in cache at /root/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.d423bdf2f58dc8b77d5f5d18028d7ae4a72dcfd8f468e81fe979ada957a8c361\n",
            "[INFO|file_utils.py:1323] 2021-02-18 19:55:01,555 >> creating metadata file for /root/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.d423bdf2f58dc8b77d5f5d18028d7ae4a72dcfd8f468e81fe979ada957a8c361\n",
            "[INFO|configuration_utils.py:456] 2021-02-18 19:55:01,555 >> loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.d423bdf2f58dc8b77d5f5d18028d7ae4a72dcfd8f468e81fe979ada957a8c361\n",
            "[INFO|configuration_utils.py:492] 2021-02-18 19:55:01,556 >> Model config DistilBertConfig {\n",
            "  \"activation\": \"gelu\",\n",
            "  \"architectures\": [\n",
            "    \"DistilBertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.1,\n",
            "  \"dim\": 768,\n",
            "  \"dropout\": 0.1,\n",
            "  \"hidden_dim\": 3072,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\",\n",
            "    \"3\": \"LABEL_3\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2,\n",
            "    \"LABEL_3\": 3\n",
            "  },\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"distilbert\",\n",
            "  \"n_heads\": 12,\n",
            "  \"n_layers\": 6,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"qa_dropout\": 0.1,\n",
            "  \"seq_classif_dropout\": 0.2,\n",
            "  \"sinusoidal_pos_embds\": false,\n",
            "  \"tie_weights_\": true,\n",
            "  \"transformers_version\": \"4.4.0.dev0\",\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "[INFO|file_utils.py:1316] 2021-02-18 19:55:01,829 >> https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpg4wyeq23\n",
            "Downloading: 100% 268M/268M [00:05<00:00, 47.5MB/s]\n",
            "[INFO|file_utils.py:1320] 2021-02-18 19:55:07,578 >> storing https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin in cache at /root/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\n",
            "[INFO|file_utils.py:1323] 2021-02-18 19:55:07,578 >> creating metadata file for /root/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\n",
            "[INFO|modeling_utils.py:1035] 2021-02-18 19:55:07,579 >> loading weights file https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\n",
            "[WARNING|modeling_utils.py:1143] 2021-02-18 19:55:09,558 >> Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\n",
            "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[WARNING|modeling_utils.py:1154] 2021-02-18 19:55:09,558 >> Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[INFO|configuration_utils.py:456] 2021-02-18 19:55:09,833 >> loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.d423bdf2f58dc8b77d5f5d18028d7ae4a72dcfd8f468e81fe979ada957a8c361\n",
            "[INFO|configuration_utils.py:492] 2021-02-18 19:55:09,834 >> Model config DistilBertConfig {\n",
            "  \"activation\": \"gelu\",\n",
            "  \"architectures\": [\n",
            "    \"DistilBertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.1,\n",
            "  \"dim\": 768,\n",
            "  \"dropout\": 0.1,\n",
            "  \"hidden_dim\": 3072,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"distilbert\",\n",
            "  \"n_heads\": 12,\n",
            "  \"n_layers\": 6,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"qa_dropout\": 0.1,\n",
            "  \"seq_classif_dropout\": 0.2,\n",
            "  \"sinusoidal_pos_embds\": false,\n",
            "  \"tie_weights_\": true,\n",
            "  \"transformers_version\": \"4.4.0.dev0\",\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "[INFO|file_utils.py:1316] 2021-02-18 19:55:10,114 >> https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpe4dowx0u\n",
            "Downloading: 100% 232k/232k [00:00<00:00, 685kB/s]\n",
            "[INFO|file_utils.py:1320] 2021-02-18 19:55:10,740 >> storing https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt in cache at /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
            "[INFO|file_utils.py:1323] 2021-02-18 19:55:10,740 >> creating metadata file for /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
            "[INFO|file_utils.py:1316] 2021-02-18 19:55:11,013 >> https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp9t1vvba1\n",
            "Downloading: 100% 466k/466k [00:00<00:00, 1.11MB/s]\n",
            "[INFO|file_utils.py:1320] 2021-02-18 19:55:11,705 >> storing https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json in cache at /root/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
            "[INFO|file_utils.py:1323] 2021-02-18 19:55:11,706 >> creating metadata file for /root/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
            "[INFO|tokenization_utils_base.py:1786] 2021-02-18 19:55:11,706 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
            "[INFO|tokenization_utils_base.py:1786] 2021-02-18 19:55:11,706 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
            "100% 120000/120000 [00:39<00:00, 3064.84ex/s]\n",
            "[INFO|trainer.py:440] 2021-02-18 19:55:51,089 >> The following columns in the training set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text.\n",
            "02/18/2021 19:55:51 - INFO - __main__ -   Training student model on teacher predictions\n",
            "[INFO|trainer.py:855] 2021-02-18 19:55:51,093 >> ***** Running training *****\n",
            "[INFO|trainer.py:856] 2021-02-18 19:55:51,093 >>   Num examples = 120000\n",
            "[INFO|trainer.py:857] 2021-02-18 19:55:51,093 >>   Num Epochs = 1\n",
            "[INFO|trainer.py:858] 2021-02-18 19:55:51,093 >>   Instantaneous batch size per device = 32\n",
            "[INFO|trainer.py:859] 2021-02-18 19:55:51,093 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
            "[INFO|trainer.py:860] 2021-02-18 19:55:51,093 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:861] 2021-02-18 19:55:51,093 >>   Total optimization steps = 3750\n",
            "{'loss': 1.0356, 'learning_rate': 4.3333333333333334e-05, 'epoch': 0.13}\n",
            " 13% 500/3750 [01:21<08:03,  6.73it/s][INFO|trainer.py:1438] 2021-02-18 19:57:12,444 >> Saving model checkpoint to ./agnews/distilled_model/checkpoint-500\n",
            "[INFO|configuration_utils.py:311] 2021-02-18 19:57:12,448 >> Configuration saved in ./agnews/distilled_model/checkpoint-500/config.json\n",
            "[INFO|modeling_utils.py:825] 2021-02-18 19:57:13,427 >> Model weights saved in ./agnews/distilled_model/checkpoint-500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1980] 2021-02-18 19:57:13,428 >> tokenizer config file saved in ./agnews/distilled_model/checkpoint-500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1986] 2021-02-18 19:57:13,429 >> Special tokens file saved in ./agnews/distilled_model/checkpoint-500/special_tokens_map.json\n",
            "{'loss': 1.0139, 'learning_rate': 3.6666666666666666e-05, 'epoch': 0.27}\n",
            " 27% 1000/3750 [02:45<07:24,  6.19it/s][INFO|trainer.py:1438] 2021-02-18 19:58:37,104 >> Saving model checkpoint to ./agnews/distilled_model/checkpoint-1000\n",
            "[INFO|configuration_utils.py:311] 2021-02-18 19:58:37,106 >> Configuration saved in ./agnews/distilled_model/checkpoint-1000/config.json\n",
            "[INFO|modeling_utils.py:825] 2021-02-18 19:58:38,055 >> Model weights saved in ./agnews/distilled_model/checkpoint-1000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1980] 2021-02-18 19:58:38,056 >> tokenizer config file saved in ./agnews/distilled_model/checkpoint-1000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1986] 2021-02-18 19:58:38,056 >> Special tokens file saved in ./agnews/distilled_model/checkpoint-1000/special_tokens_map.json\n",
            "{'loss': 1.0055, 'learning_rate': 3e-05, 'epoch': 0.4}\n",
            " 40% 1500/3750 [04:11<07:34,  4.95it/s][INFO|trainer.py:1438] 2021-02-18 20:00:02,277 >> Saving model checkpoint to ./agnews/distilled_model/checkpoint-1500\n",
            "[INFO|configuration_utils.py:311] 2021-02-18 20:00:02,279 >> Configuration saved in ./agnews/distilled_model/checkpoint-1500/config.json\n",
            "[INFO|modeling_utils.py:825] 2021-02-18 20:00:03,213 >> Model weights saved in ./agnews/distilled_model/checkpoint-1500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1980] 2021-02-18 20:00:03,214 >> tokenizer config file saved in ./agnews/distilled_model/checkpoint-1500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1986] 2021-02-18 20:00:03,214 >> Special tokens file saved in ./agnews/distilled_model/checkpoint-1500/special_tokens_map.json\n",
            "{'loss': 1.0018, 'learning_rate': 2.3333333333333336e-05, 'epoch': 0.53}\n",
            " 53% 2000/3750 [05:35<04:17,  6.80it/s][INFO|trainer.py:1438] 2021-02-18 20:01:26,659 >> Saving model checkpoint to ./agnews/distilled_model/checkpoint-2000\n",
            "[INFO|configuration_utils.py:311] 2021-02-18 20:01:26,660 >> Configuration saved in ./agnews/distilled_model/checkpoint-2000/config.json\n",
            "[INFO|modeling_utils.py:825] 2021-02-18 20:01:27,513 >> Model weights saved in ./agnews/distilled_model/checkpoint-2000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1980] 2021-02-18 20:01:27,514 >> tokenizer config file saved in ./agnews/distilled_model/checkpoint-2000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1986] 2021-02-18 20:01:27,514 >> Special tokens file saved in ./agnews/distilled_model/checkpoint-2000/special_tokens_map.json\n",
            "{'loss': 1.0019, 'learning_rate': 1.6666666666666667e-05, 'epoch': 0.67}\n",
            " 67% 2500/3750 [06:59<03:13,  6.45it/s][INFO|trainer.py:1438] 2021-02-18 20:02:50,641 >> Saving model checkpoint to ./agnews/distilled_model/checkpoint-2500\n",
            "[INFO|configuration_utils.py:311] 2021-02-18 20:02:50,643 >> Configuration saved in ./agnews/distilled_model/checkpoint-2500/config.json\n",
            "[INFO|modeling_utils.py:825] 2021-02-18 20:02:51,591 >> Model weights saved in ./agnews/distilled_model/checkpoint-2500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1980] 2021-02-18 20:02:51,592 >> tokenizer config file saved in ./agnews/distilled_model/checkpoint-2500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1986] 2021-02-18 20:02:51,592 >> Special tokens file saved in ./agnews/distilled_model/checkpoint-2500/special_tokens_map.json\n",
            "{'loss': 0.9948, 'learning_rate': 1e-05, 'epoch': 0.8}\n",
            " 80% 3000/3750 [08:24<02:16,  5.50it/s][INFO|trainer.py:1438] 2021-02-18 20:04:15,807 >> Saving model checkpoint to ./agnews/distilled_model/checkpoint-3000\n",
            "[INFO|configuration_utils.py:311] 2021-02-18 20:04:15,809 >> Configuration saved in ./agnews/distilled_model/checkpoint-3000/config.json\n",
            "[INFO|modeling_utils.py:825] 2021-02-18 20:04:16,695 >> Model weights saved in ./agnews/distilled_model/checkpoint-3000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1980] 2021-02-18 20:04:16,695 >> tokenizer config file saved in ./agnews/distilled_model/checkpoint-3000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1986] 2021-02-18 20:04:16,696 >> Special tokens file saved in ./agnews/distilled_model/checkpoint-3000/special_tokens_map.json\n",
            "{'loss': 0.9951, 'learning_rate': 3.3333333333333333e-06, 'epoch': 0.93}\n",
            " 93% 3500/3750 [09:51<00:37,  6.63it/s][INFO|trainer.py:1438] 2021-02-18 20:05:42,415 >> Saving model checkpoint to ./agnews/distilled_model/checkpoint-3500\n",
            "[INFO|configuration_utils.py:311] 2021-02-18 20:05:42,416 >> Configuration saved in ./agnews/distilled_model/checkpoint-3500/config.json\n",
            "[INFO|modeling_utils.py:825] 2021-02-18 20:05:43,414 >> Model weights saved in ./agnews/distilled_model/checkpoint-3500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1980] 2021-02-18 20:05:43,422 >> tokenizer config file saved in ./agnews/distilled_model/checkpoint-3500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1986] 2021-02-18 20:05:43,423 >> Special tokens file saved in ./agnews/distilled_model/checkpoint-3500/special_tokens_map.json\n",
            "100% 3750/3750 [10:35<00:00,  6.34it/s][INFO|trainer.py:1029] 2021-02-18 20:06:26,292 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 635.1987, 'train_samples_per_second': 5.904, 'epoch': 1.0}\n",
            "100% 3750/3750 [10:35<00:00,  5.90it/s]\n",
            "[INFO|trainer.py:440] 2021-02-18 20:06:26,307 >> The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text.\n",
            "[INFO|trainer.py:1630] 2021-02-18 20:06:26,313 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:1631] 2021-02-18 20:06:26,313 >>   Num examples = 120000\n",
            "[INFO|trainer.py:1632] 2021-02-18 20:06:26,314 >>   Batch size = 128\n",
            "100% 938/938 [03:29<00:00,  4.48it/s]\n",
            "02/18/2021 20:09:55 - INFO - __main__ -   Agreement of student and teacher predictions: 92.05%\n",
            "[INFO|trainer.py:1438] 2021-02-18 20:09:55,992 >> Saving model checkpoint to ./agnews/distilled_model\n",
            "[INFO|configuration_utils.py:311] 2021-02-18 20:09:55,994 >> Configuration saved in ./agnews/distilled_model/config.json\n",
            "[INFO|modeling_utils.py:825] 2021-02-18 20:09:56,978 >> Model weights saved in ./agnews/distilled_model/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1980] 2021-02-18 20:09:56,979 >> tokenizer config file saved in ./agnews/distilled_model/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1986] 2021-02-18 20:09:56,979 >> Special tokens file saved in ./agnews/distilled_model/special_tokens_map.json\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3d2-ptlaF2M5"
      },
      "source": [
        "### Using the student model\n",
        "\n",
        "The resulting model can now be loaded and used like any other pre-trained model:\n",
        "\n",
        "(you can also use `\"joeddav/distilbert-base-uncased-agnews-student\"` to download this model from the hub if you want to try it without running whole script above)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hwh61MsAGdEw",
        "outputId": "5ec862e9-4a6c-4e6e-ecf2-a768c9563be8"
      },
      "source": [
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"./distilbert-base-uncased-agnews-student\")\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"./distilbert-base-uncased-agnews-student\")\n",
        "model.config"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DistilBertConfig {\n",
              "  \"_name_or_path\": \"joeddav/distilbert-base-uncased-agnews-student\",\n",
              "  \"activation\": \"gelu\",\n",
              "  \"architectures\": [\n",
              "    \"DistilBertForSequenceClassification\"\n",
              "  ],\n",
              "  \"attention_dropout\": 0.1,\n",
              "  \"dim\": 768,\n",
              "  \"dropout\": 0.1,\n",
              "  \"hidden_dim\": 3072,\n",
              "  \"id2label\": {\n",
              "    \"0\": \"the world\",\n",
              "    \"1\": \"sports\",\n",
              "    \"2\": \"business\",\n",
              "    \"3\": \"science/tech\"\n",
              "  },\n",
              "  \"initializer_range\": 0.02,\n",
              "  \"label2id\": {\n",
              "    \"business\": 2,\n",
              "    \"science/tech\": 3,\n",
              "    \"sports\": 1,\n",
              "    \"the world\": 0\n",
              "  },\n",
              "  \"max_position_embeddings\": 512,\n",
              "  \"model_type\": \"distilbert\",\n",
              "  \"n_heads\": 12,\n",
              "  \"n_layers\": 6,\n",
              "  \"pad_token_id\": 0,\n",
              "  \"qa_dropout\": 0.1,\n",
              "  \"seq_classif_dropout\": 0.2,\n",
              "  \"sinusoidal_pos_embds\": false,\n",
              "  \"tie_weights_\": true,\n",
              "  \"transformers_version\": \"4.3.2\",\n",
              "  \"vocab_size\": 30522\n",
              "}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "glxrI7uXGhI6"
      },
      "source": [
        "and even used trivially with a `TextClassificationPipeline`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dAKVEuPEGXzF",
        "outputId": "b2f1a0be-9dbc-4a2d-b53d-e9677b5b2841"
      },
      "source": [
        "from transformers import TextClassificationPipeline\n",
        "distilled_classifier = TextClassificationPipeline(model=model, tokenizer=tokenizer, return_all_scores=True, device=0)\n",
        "distilled_classifier(sequence)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[{'label': 'the world', 'score': 0.1478172093629837},\n",
              "  {'label': 'sports', 'score': 0.03247390314936638},\n",
              "  {'label': 'business', 'score': 0.059586379677057266},\n",
              "  {'label': 'science/tech', 'score': 0.7601224780082703}]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WoQ6hQe2Miu4"
      },
      "source": [
        "Let's compare the speed & accuracy of the two methods.\n",
        "\n",
        "Original zero-shot model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101,
          "referenced_widgets": [
            "65e7c2a2bfa84bb5af462e7525a2c8db",
            "39b11c030de449bc892afa1f1e5f36e2",
            "8b8583289b2a40e2b67d073f6a20507d",
            "90386ac483ca4e8983fc273c2ab4126c",
            "b89937e4985e49feaf68d03578043a90",
            "cfa8908bcc924a949db98fa53be4540f",
            "c48bd1b53b1b4206adaf20d0c0759e93",
            "61f3ff8b29014543be0bc2bb8be2ab87"
          ]
        },
        "id": "to7DOOp9LxlM",
        "outputId": "4b474035-3314-4374-f88f-0b61d9bf62dc"
      },
      "source": [
        "import numpy as np\n",
        "from time import time\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "start = time()\n",
        "batch_size = 32\n",
        "hypothesis_template = \"This text is about {}.\"\n",
        "preds = []\n",
        "for i in tqdm(range(0, len(test), batch_size)):\n",
        "    examples = test[i:i+batch_size]['text']\n",
        "    outputs = zero_shot_classifier(examples, class_names, hypothesis_template=hypothesis_template)\n",
        "    preds += [class_names.index(o['labels'][0]) for o in outputs]\n",
        "accuracy = np.mean(np.array(preds) == np.array(test['label']))\n",
        "print(f\"Teacher model accuracy: {accuracy*100:0.2f}%\")\n",
        "print(f\"Runtime: {time() - start : 0.2f} seconds\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "65e7c2a2bfa84bb5af462e7525a2c8db",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=238.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Zero-shot model accuracy: 69.33%\n",
            "Runtime:  195.19 seconds\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o74qNjt-MOCI"
      },
      "source": [
        "Distilled student model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r4tuVf6VNLGg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101,
          "referenced_widgets": [
            "899620103bc4450fbca7e59e45dcbc81",
            "0fbd3a194b284c0f87c1fac48ec0d023",
            "3030a8a6822c4ef78e5a0a0b8a7a0049",
            "ca78f7fab6854d2bb57747f966bf66bd",
            "253cbb88dc904c228a23217f5ef76f53",
            "a98d0be96c5b4252b3c9c3954843397d",
            "ec04d21385fc4332b7ac285b9c47dca9",
            "589eee9a1f414760b11587d4cbcb8943"
          ]
        },
        "outputId": "fdb4e053-c963-4a6b-c49d-98f3db514531"
      },
      "source": [
        "start = time()\n",
        "batch_size = 128 # larger batch size bc distilled model is more memory efficient\n",
        "distilled_classifier.return_all_scores = False\n",
        "preds = []\n",
        "for i in tqdm(range(0, len(test), batch_size)):\n",
        "    examples = test[i:i+batch_size]['text']\n",
        "    outputs = distilled_classifier(examples)\n",
        "    preds += [class_names.index(o['label']) for o in outputs]\n",
        "accuracy = np.mean(np.array(preds) == np.array(test['label']))\n",
        "print(f\"Distilled model accuracy: {accuracy*100:0.2f}%\")\n",
        "print(f\"Runtime: {time() - start : 0.2f} seconds\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "899620103bc4450fbca7e59e45dcbc81",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=60.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Distilled model accuracy: 70.47%\n",
            "Runtime:  11.27 seconds\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Uhs--uSnuaf"
      },
      "source": [
        "As you can see, **the disitlled model gets similar accuracy on a held-out test set while running in 1/20th the time**. \n",
        "\n",
        "Lastly, you can share the distilled model with the community by [uploading it to the ðŸ¤— Hub](https://huggingface.co/transformers/model_sharing.html). We've uploaded the distilled model from this notebook at [joeddav/distilbert-base-uncased-agnews-student](https://huggingface.co/joeddav/distilbert-base-uncased-agnews-student)."
      ]
    }
  ]
}